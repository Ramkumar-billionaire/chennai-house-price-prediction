# -*- coding: utf-8 -*-
"""Final_Chennai_House_Price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dvXeaGBSZALGduwpXJ9MMmpBbfJaz45i

# ☛ Problem Statement :
<pre>Real estate transactions are quite opaque sometimes and it may be  difficult for a newbie to know the fair price of any given home.
Thus, multiple real estate websites have the functionality to predict the prices of houses given different features regarding it.
Such forecasting models will help buyers to identify a fair price for the home and also give insights to sellers as to how to build
homes that fetch them more money.
It is expected to build a sale price prediction model that will aid the customers to find a fair price for their homes
and also help the sellers understand what factors are fetching more money for the houses.</pre>

# ≣ Variables in the Dataset :
       ✤ PRT_ID : Project ID (object)
       ✤ AREA : Area where the house is located (object)
       ✤ INT_SQFT : Total area of the house in square-feet (int)
       ✤ DATE_SALE : Date on which the house got sold (object)
       ✤ DIST_MAINROAD : Distance from the house to the mainroad -in meters(int)
       ✤ N_BEDROOM : Number of Bedrooms (float)
       ✤ N_BATHROOM : Number of Bathrooms (float)
       ✤ N_ROOM : Number of Rooms (int)
       ✤ SALE_COND : Sale Conditions (object)
       ✤ PARK_FACIL : Parking Facility (object)
       ✤ DATE_BUILD : Date on which the house was built (object)
       ✤ BUILD_TYPE : Type of the house (object)
       ✤ UTILITY_AVAIL : Utilities available for the owner of the house (object)
       ✤ STREET : Street where the house is located (object)
       ✤ MZZONE : Chennai Regions are divided into multiple zones, MZZONE is nothing but the zone where the house belongs to (object)
       ✤ QS_ROOMS, QS_BATHROOM, QS_BEDROOM, QS_OVERALL : Masked Data (float)
       ✤ REG_FEE : Registration Fees (int)
       ✤ COMMIS : Commission (int)
       ✤ SALE_PRICE : Price at which the house got sold (int)

## Importing necessary dependencies
"""

!pip install seaborn==0.11

# for data reading and data manipulation
import numpy as np
import pandas as pd
import statistics as st

# for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# for model creation and model evaluation
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

# reading data from a .csv file to a Pandas DataFrame
df = pd.read_csv('train-chennai-sale.csv')
pd.set_option('display.max_columns',None)
df.head()

df.shape

df.columns

"""<pre>As the names of all the columns in the DataFrame are in CAPS, I would like to rename all the columns to it's lower_case form,
So later it would be efficient for me to work with the dataframe. For this I will create a function 'rename_cols'.</pre>
"""

df.columns = [column.lower() for column in df.columns]

df.columns

df.head()

print(f'➤ The DataFrame(df) contains {df.shape[0]} rows and {df.shape[1]} columns.')

"""<pre>Here I can clearly sense that there are 3 as of such columns which in no way can contribute for the price prediction of house :
1.) prt_id
2.) reg_fee
3.) commis

Hence it is better to drop these 3 columns before further analysis.</pre>
"""

df.drop(columns = ['prt_id','reg_fee','commis'],inplace=True)

print(f'➤ Now the shape of the Dataframe is {df.shape[0]} rows and {df.shape[1]} columns.')

df.head(2)

"""## Checking and Handling Missing Values
    Checking for missing values
"""

# looking for the amount of null data in the pandas dataframe
df.isnull().sum()

# looking for the percentage of null data in the dataframe
pd.DataFrame(df.isna().mean()*100).T

"""Handling missing values.

<pre>The percentage of missing values for columns with missing values is way less then 5%,
Therefore we can deal with them rather than losing our data.</pre>

<pre>➺ As there is only 1 missing value in <i>n_bedroom</i>, it can be replaced with Mode.
➺ There are 5 missing values in <i>n_bathroom</i>, it  can be replaced with the median of that particular column.
➺ The columns <i>qs_overall</i> is a special case as the data in that column is masked and the data is in floating point number,
it is good to replace it with mean.</pre>
"""

st.mean(df['qs_overall']), st.median(df['qs_overall'])

# filling the missing values using statistical techniques
df = df.fillna({'n_bedroom': st.mode(df['n_bedroom']),
                'n_bathroom': st.mode(df['n_bathroom']),
                'qs_overall': df['qs_overall'].mean()})

# cross  checking  for  null  values
df.isnull().sum()

"""## Data Type Correction
<pre>It is observed that the data-type of certain columns are not exactly the right data-type suitable for those corresponding columns:
⃰ The data type of columns <i>n_bedroom</i> and <i>n_bathroom</i> are unnecessarily given as 'float' but it is better to change them into 'int' for better computation.
⃰ The data type of columns <i>date_sale</i> and <i>date_build</i> are as 'strings' but should be of 'datetime'.


"""

df.dtypes

# correcting data type of columns n_bedroom and n_bathroom
df['n_bedroom'] = df['n_bedroom'].astype(int)
df['n_bathroom'] = df['n_bathroom'].astype(int)

#correcting data type of columns date_sale and date_build
df['date_sale'] = pd.to_datetime(df['date_sale'])
df['date_build'] = pd.to_datetime(df['date_build'])

from datetime import date, datetime, timedelta

todayData = datetime(2023, 2, 18,4,48,0)
todayData.weekday()

# cross-checking the data-types of all the columns in the dataframe
df.dtypes

"""## Creation of a new column "house_age":
There are 2 columns named 'date_sale' and 'date_build' which represents date on which the house got sold and the date on which the house was built respectively. Both of this columns do not make any sense as of such but there is one thing that can be yielded using both of this columns and that is age of the house, how old the house is can be a good decisive factor for determining the "sales_price". This new column's value will be in *no_of_years*.

After this we can drop columns *date_sale* and *date_build*.

<pre> house_age = date_sale - date_build</pre>
"""

df['house_age'] = df['date_sale'].dt.year - df['date_build'].dt.year

df['house_age']

df.drop(columns = ['date_sale','date_build'],inplace=True)

df.head()

df.shape

# viewing how many columns in the dataframe fall in the object and the numeric data types
pd.DataFrame(df.dtypes.value_counts()).T

integer_data_cols = [var for var in df.columns if df[var].dtype == 'int64']
integer_data_cols

"""    From the above results of the code cell it seems that there is nothing to worry about the int64 cols no changes needed."""

float_data_cols = [var for var in df.columns if df[var].dtype == 'float64']
float_data_cols

"""    From the above results of columns with floating point number there seems nothing wrong, No changes needed."""

object_data_cols = [var for var in df.columns if df[var].dtype == 'object']
object_data_cols

"""<pre>As the number of object data type columns is more it is important to know how each object data column is divided into categories.
Let's explore all the categorical columns and know into how many categories they are divided into.</pre>
"""

for i in object_data_cols:
  print(f'Column "{i}" is divided into "{len(df[i].value_counts())}" categories.')

"""The categorical columns are having too many categories, seems like it is needed to analyze all of them one by one."""

# for column "area"
df['area'].value_counts()

"""<pre>For the column "area" there are many spelling mistakes which is the resulting factor for soo many categories.
Let's fix 'em </pre>
"""

df['area'] = df['area'].replace({'Chrompt':'Chrompet','Chrmpet':'Chrompet','Chormpet':'Chrompet','TNagar':'T Nagar','Karapakam':'Karapakkam','Ana Nagar':'Anna Nagar','Ann Nagar':'Anna Nagar','Velchery':'Velachery','Adyr':'Adyar','KKNagar':'KK Nagar'})

df['area'].value_counts()

# for column "sale_cond"
df['sale_cond'].value_counts()

"""<pre>Even here we have the same spelling mistakes.
Let's fix 'em up!</pre>
"""

df['sale_cond'] = df['sale_cond'].replace({'Adj Land':'AdjLand','Ab Normal':'AbNormal','Partiall':'Partial','PartiaLl':'Partial'})

df['sale_cond'].value_counts()

# for column "park_facil"
df['park_facil'].value_counts()

"""<pre> Same Spelling Mistakes!!!!!</pre>"""

df['park_facil'] = df['park_facil'].replace({'Noo':'No'})

df['park_facil'].value_counts()

# for column "buildtype"
df['buildtype'].value_counts()

"""<pre>AGHHH!!! Same spelling mistakes!"""

df['buildtype'] = df['buildtype'].replace({'Comercial':'Commercial','Other':'Others'})

df['buildtype'].value_counts()

# for column "utility_avail"
df['utility_avail'].value_counts()

"""<pre> Again its the same thing here,SPELLING MISTAKES!!! </pre>"""

df['utility_avail'] = df['utility_avail'].replace(['NoSewr ','NoSeWa','All Pub'],['No Sewer','No Sewer','AllPub'])

df['utility_avail'].value_counts()

# for column "street"
df['street'].value_counts()

"""<pre> Spelling Mistake Again! </pre>"""

df['street'] = df['street'].replace({'Pavd':'Paved','NoAccess':'No Access'})

df['street'].value_counts()

# for column "mzzone"
df['mzzone'].value_counts()

"""<pre> Here in column "mzzone" there is no such spelling error, everything here is okay!</pre>"""

df = df.drop_duplicates()

df.shape

"""## Data Visualization."""

df['area'].value_counts().index[::-1]

df['area'].value_counts().index[::-1]

#plotting count plots for all the categorical columns
sns.set_theme(style='darkgrid',palette='pastel')

plt.figure(figsize=(20,25))
plt.subplot(431)
sns.countplot(df['area'],order=df['area'].value_counts().index[::-1])
plt.xticks(rotation=15)
plt.xlabel('Area')
plt.ylabel('Count')
plt.title('Sales of houses according to Area')

plt.subplot(432)
sns.countplot(df['sale_cond'],order=df['sale_cond'].value_counts().index[::-1])
plt.xticks(rotation=15)
plt.xlabel('Sale_Cond')
plt.ylabel('Count')
plt.title('Sales of houses according to Sale Conditions')

plt.subplot(433)
sns.countplot(df['park_facil'],order=df['park_facil'].value_counts().index[::-1])
plt.xticks(rotation=15)
plt.xlabel('Parking Facility')
plt.ylabel('Count')
plt.title('Sale of houses according to Parking Facility')

plt.subplot(434)
sns.countplot(df['buildtype'],order=df['buildtype'].value_counts().index[::-1])
plt.xticks(rotation=15)
plt.xlabel('Buildtype')
plt.ylabel('Count')
plt.title('Sales of houses according to Buildtype')

plt.subplot(435)
sns.countplot(df['utility_avail'],order=df['utility_avail'].value_counts().index[::-1])
plt.xticks(rotation=15)
plt.xlabel('Utilities Available')
plt.ylabel('Count')
plt.title('Sales of houses according to Utilities Available')

plt.subplot(436)
sns.countplot(df['street'],order=df['street'].value_counts().index[::-1])
plt.xticks(rotation=15)
plt.xlabel('Street')
plt.ylabel('Count')
plt.title('Sales of houses according to Street')

plt.subplot(437)
sns.countplot(df['mzzone'],order=df['mzzone'].value_counts().index[::-1])
plt.xticks(rotation=15)
plt.xlabel('MZZONE')
plt.ylabel('Count')
plt.title('Sales of houses according to MZZONES')

plt.tight_layout()

"""<pre>Interpretations :
 1.) According to the column "area" the highest number of houses sold were in the area "Chrompet".
 2.) According to the column "sale_cond", no such category has a strong influence on sales of house.
 3.) According to the column "park_facil", houses with parking facilities are slightly sold more.
 4.) According to the column "buildtype", houses used for living are sold more then using it as commercial or other spaces.
 5.) According to the column "utility_avail", houses with No Sewer are most sold.
 6.) According to the column "street", paved street and gravel street are more sold compared to houses with no access to street.
 7.) According to the column "mzzone", Zones such as RL, RH, RM are the zones where the most number of houses got sold.</pre>

Plotting barplots for Categorical Columns vs Mean Sales Price.
"""

plt.figure(figsize=(20,25))

plt.subplot(431)
sns.barplot(df['area'],df['sales_price'], order = df.groupby('area')['sales_price'].mean().reset_index().sort_values('sales_price')['area'])
plt.xlabel('Area')
plt.ylabel('Mean Sales Price')
plt.title('Area vs Mean_Sales_Price')



#orders for all the barplots in ascending order
area_order = df.groupby('area')['sales_price'].mean().reset_index().sort_values('sales_price')['area']
sale_cond_order = df.groupby('sale_cond')['sales_price'].mean().reset_index().sort_values('sales_price')['sale_cond']
park_facil_order = df.groupby('park_facil')['sales_price'].mean().reset_index().sort_values('sales_price')['park_facil']
buildtype_order = df.groupby('buildtype')['sales_price'].mean().reset_index().sort_values('sales_price')['buildtype']
utility_avail_order = df.groupby('utility_avail')['sales_price'].mean().reset_index().sort_values('sales_price')['utility_avail']
street_order = df.groupby('street')['sales_price'].mean().reset_index().sort_values('sales_price')['street']
mzzone_order = df.groupby('mzzone')['sales_price'].mean().reset_index().sort_values('sales_price')['mzzone']

df.groupby('area')['sales_price'].mean().reset_index().sort_values('sales_price')['area'].values

#for categorical columns
plt.figure(figsize=(20,25))

plt.subplot(431)
sns.barplot(df['area'],df['sales_price'], order = area_order)
plt.xlabel('Area')
plt.ylabel('Mean Sales Price')
plt.title('Area vs Mean_Sales_Price')

plt.subplot(432)
sns.barplot(df['sale_cond'],df['sales_price'],order=sale_cond_order)
plt.xlabel('Sale Conditions')
plt.ylabel('Mean Sales Price')
plt.title('Sale_Conditions vs Mean_Sales_Price')

plt.subplot(433)
sns.barplot(df['park_facil'],df['sales_price'],order=park_facil_order)
plt.xlabel('Parking Facility')
plt.ylabel('Mean Sales Price')
plt.title('Parking Facility vs Mean_Sales_Price')

plt.subplot(434)
sns.barplot(df['buildtype'],df['sales_price'],order=buildtype_order)
plt.xlabel('Build-type')
plt.ylabel('Mean Sales Price')
plt.title('House Buildtype vs Mean_Sales_Price')

plt.subplot(435)
sns.barplot(df['utility_avail'],df['sales_price'],order=utility_avail_order)
plt.xlabel('Utilities Available')
plt.ylabel('Mean Sales Price')
plt.title('Utilities Available vs Mean_Sales_Price')

plt.subplot(436)
sns.barplot(df['street'],df['sales_price'],order=street_order)
plt.xlabel('Street')
plt.ylabel('Mean Sales Price')
plt.title('Street vs Mean_Sales_Price')

plt.subplot(437)
sns.barplot(df['mzzone'],df['sales_price'],order=mzzone_order)
plt.xlabel('MZZONE')
plt.ylabel('Mean Sales Price')
plt.title('MZZONs vs Mean_Sales_Price')

plt.tight_layout()

"""<pre>Interpretations :
From the above data visualization it can be understood that the categorical columns do have linear realtionship with the
target variable ("sales_price" here).
So we have to use Ordinal Encoding to Encode all these categorical columns.</pre>

Plotting line plots for integer column vs Sales Price column.
"""

plt.figure(figsize=(20,25))
plt.subplot(331)
sns.scatterplot(df['int_sqft'],df['sales_price'])
plt.title('Total Area in Sqft vs Sales_Price')
plt.subplot(332)
sns.scatterplot(df['dist_mainroad'],df['sales_price'])
plt.title('Distance from Mainroad vs Sales_Price')
plt.subplot(333)
sns.lineplot(df['n_bedroom'],df['sales_price'])
plt.title('Number of Bedrooms vs Sales_Price')
plt.subplot(334)
sns.lineplot(df['n_bathroom'],df['sales_price'])
plt.title('Number of Bathrooms vs Sales_Price')
plt.subplot(335)
sns.lineplot(df['n_room'],df['sales_price'])
plt.title('Number of Rooms vs Sales_Price')
plt.subplot(336)
sns.lineplot(df['house_age'],df['sales_price'])
plt.title('House Age vs Sales_Price')

"""<pre>Interpretations :

For columns 'int_sqft','n_bedroom,'n_bathroom,'n_room' there is a clear Linear Relationship with the 'Sales_Price' column.

For column 'dist_mainroad' the distribution is uniform, no such linear relationship is discovered here.

For column 'house_age' we can see a hybrid kinda behaviour. For house ages 10 to 40 the sales_price drops a bit,
but from 40 to ~43 house prices rises and then again falls from ~43 - 50 and again it rises from 50-...

Plotting scatter plots for masked data vs the sales_price column
"""

plt.figure(figsize=(15,8))

plt.subplot(221)
sns.lineplot(df['qs_rooms'],df['sales_price'])
plt.xlabel('Qs Rooms')
plt.ylabel('Sales Price')
plt.title('Qs Rooms vs Sales Price')

plt.subplot(222)
sns.lineplot(df['qs_bedroom'],df['sales_price'])
plt.xlabel('Qs Bedroom')
plt.ylabel('Sales Price')
plt.title('Qs Bedroom vs Sales Price')

plt.subplot(223)
sns.lineplot(df['qs_bathroom'],df['sales_price'])
plt.xlabel('Qs Bathroom')
plt.ylabel('Sales Price')
plt.title('Qs Bathroom vs Sales Price')

plt.subplot(224)
sns.lineplot(df['qs_overall'],df['sales_price'])
plt.xlabel('Qs Overall')
plt.ylabel('Sales Price')
plt.title('Qs Overall vs Sales Price')

plt.tight_layout()

"""<pre>Interpretations :
From the above scatterplots we can clearly observe that none of the plots exihibits Linear Relationship with the
Target Variable i.e Sales_Price.
"""

del df['qs_rooms']
del df['qs_overall']
del df['qs_bathroom']
del df['qs_bedroom']
del df['dist_mainroad']
del df['sale_cond']
del df['utility_avail']

"""## Encoding Categorical Variables"""

df['area'] = df['area'].map({'Karapakkam':0,'Adyar':1,'Chrompet':2,'Velachery':3,'KK Nagar':4,'Anna Nagar':5,'T Nagar':6})
df['street'] = df['street'].map({'No Access':0,'Paved':1,'Gravel':2})
df['mzzone'] = df['mzzone'].map({'A':0,'C':1,'I':2,'RH':4,'RL':5,'RM':6})
df['park_facil'] = df['park_facil'].map({'No':0,'Yes':1})
df = pd.get_dummies(df, columns = ['buildtype'])

df.head()

"""## Splitting the data into input data and output data"""

X = df.drop('sales_price',axis=1)
y = df['sales_price']

X

y

#splitting the data into training and testing sets with the ratio of 8:2
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=70)

print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

X_train

"""##**12.Building Machine Learning Model**"""

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

"""###**12.1 Linear regression**"""

from sklearn.linear_model import LinearRegression #import
linear_model = LinearRegression(fit_intercept=True) #initialise
linear_model.fit(X_train,y_train) #fit - all magic
print(linear_model.predict(X_test))     #predict
print(y_test)

linear_model.score(X_test, y_test)

from sklearn.model_selection import cross_val_score
# synatx : cross_val_score(model, fts_train, target_train, bins).mean()
cross_val_linear_model=cross_val_score(linear_model,X_train,y_train,cv=10).mean()
cross_val_linear_model

"""###**12.2 K Nearest Neighbor Regression**

####**12.2.1 Choosing the best K(neighbor) Value**
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn_values=np.arange(1,50)
cross_val_knn=[]
for k in knn_values:
  knn_regressor=KNeighborsRegressor(n_neighbors=k)
  knn_regressor.fit(X_train_scaled,y_train)
  print("K value : ", k, " train score : ", knn_regressor.score(X_train_scaled,y_train)  ,"cross_val_score : ", cross_val_score(knn_regressor,X_train_scaled,y_train,cv = 10).mean())
  cross_val_knn.append(cross_val_score(knn_regressor,X_train_scaled,y_train,cv = 10).mean())

cross_val_knn_regressor=max(cross_val_knn)

print("The best K-Value is 3 and Cross_val_score is",cross_val_knn_regressor )

"""####**12.2.2 Implementing K Nearest Neighbor Regression**"""

knn_regressor=KNeighborsRegressor(n_neighbors=3)
knn_regressor.fit(X_train_scaled,y_train)

cross_val_knn_regressor=cross_val_score(knn_regressor,X_train_scaled,y_train,cv=15).mean()
cross_val_knn_regressor

"""###**12.3 Decision Tree Regression**

####**12.3.1 Choosing the best of depth Value**
"""

from sklearn.tree import DecisionTreeRegressor

max_depth=np.arange(1,20)
cross_val_dt=[]
for d in max_depth:
  dt_regressor= DecisionTreeRegressor(max_depth=d, random_state=0)
  dt_regressor.fit(X_train,y_train)
  print("Depth : ", d, " train Score  : ", dt_regressor.score(X_train,y_train), "cross_val_score : ", cross_val_score(dt_regressor,X_train,y_train,cv = 10).mean())
  cross_val_dt.append(cross_val_score(dt_regressor,X_train,y_train,cv = 10).mean())

cross_val_dt_regressor=max(cross_val_dt)

print("The best depth is 14 and Cross_val_score is:",cross_val_dt_regressor)

"""####**12.3.2 Implementing Decision Tree Regression**"""

dt_regressor=DecisionTreeRegressor(max_depth=14, random_state=0)
dt_regressor.fit(X_train,y_train)

cross_val_dt_regressor=cross_val_score(dt_regressor,X_train,y_train,cv=10).mean()
cross_val_dt_regressor

ftImp = list(zip(dt_regressor.feature_importances_, df.columns[:-1]))
imp = pd.DataFrame(ftImp, columns = ["Importance","Feature"])
imp.sort_values("Importance",ascending = False,inplace=True)
imp

"""###**12.4 Random Forest Regression**

####**12.4.1 Choosing the best depth value**
"""

from sklearn.ensemble import RandomForestRegressor

max_depth=np.array([2,4,8,10,11,12,13,15,18,20])
cross_val_rf=[]
for d in max_depth:
  rf_regressor=RandomForestRegressor(max_depth=d, random_state=0)
  rf_regressor.fit(X_train,y_train)
  print("Depth : ", d, "cross_val_score : ", cross_val_score(rf_regressor,X_train,y_train,cv = 15).mean())
  cross_val_rf.append(cross_val_score(rf_regressor,X_train,y_train,cv = 15).mean())

cross_val_rf_regressor=max(cross_val_rf)

print("The best depth is 20 and Cross_val_score is:",cross_val_rf_regressor)

"""####**12.4.2 Implementing Random Forest Regression**"""

rf_regressor=RandomForestRegressor(max_depth=20, random_state=0)
rf_regressor.fit(X_train,y_train)

cross_val_rf_regressor=cross_val_score(rf_regressor,X_train,y_train,cv=15).mean()
cross_val_rf_regressor

"""###**12.6 Extreme Gradient Boosting Regression**

####**12.6.1 Choosing the best Learning Rate**
"""

import xgboost as xgb

cross_val_xgb=[]
for lr in [0.01,0.05,0.08,0.1,0.2,0.25,0.3]:
  xgb_regressor= xgb.XGBRegressor(learning_rate = lr,n_estimators=100)
  xgb_regressor.fit(X_train,y_train)
  print("Learning rate : ", lr,"cross_val_score:", cross_val_score(xgb_regressor,X_train,y_train,cv = 15).mean())
  cross_val_xgb.append(cross_val_score(xgb_regressor,X_train,y_train,cv = 15).mean())

cross_val_xgb_regressor=max(cross_val_xgb)

print("The best Learning rate is 0.1 and Cross_val_score is:",cross_val_xgb_regressor)

"""####**12.6.2 Implementing Extreme Gradient Boosting Regression**"""

xgb_regressor= xgb.XGBRegressor(learning_rate =0.1,n_estimators=100) # initialise the model
  xgb_regressor.fit(X_train,y_train) #train the model

cross_val_xgb_regressor=cross_val_score(xgb_regressor,X_train,y_train,cv=15).mean()
cross_val_xgb_regressor

"""###**12.7 Cross-Validation Score for Machine-Learning Models**"""

print("Cross Validation Score for Linear Regression Model:",cross_val_linear_model)
print("Cross Validation Score for K-Nearest Neighbors Regression Model:",cross_val_knn_regressor)
print("Cross Validation Score for Decision Tree Regression Model: ",cross_val_dt_regressor)
print("Cross Validation Score for Random Forest Regression Model: ",cross_val_rf_regressor)
print("Cross Validation Score for Extreme-Gradient Boosting Regression Model: ",cross_val_xgb_regressor)

"""###**12.8 R2 Score for Machine-Learning Models**"""

from sklearn.metrics import r2_score

y_pred_lr=linear_model.predict(X_test)
y_pred_knn=knn_regressor.predict(X_test)
y_pred_dt= dt_regressor.predict(X_test)
y_pred_rf=rf_regressor.predict(X_test)
y_pred_xgb=xgb_regressor.predict(X_test)

R2_score_lr=r2_score(y_test,y_pred_lr)
R2_score_knn=r2_score(y_test,y_pred_knn)
R2_score_dt=r2_score(y_test,y_pred_dt)
R2_score_rf=r2_score(y_test,y_pred_rf)
R2_score_xgb=r2_score(y_test,y_pred_xgb)

print("R2 Score for Linear Regression Model:",R2_score_lr)
print("R2 Score for K-Nearest Neighbors Regression Model:",R2_score_knn)
print("R2 Score for Decision Tree Regression Model: ",R2_score_dt)
print("R2 Score for Random Forest Regression Model: ",R2_score_rf)
print("R2 Score for Extreme-Gradient Boosting Regression Model: ",R2_score_xgb)

"""##**13. Suggestion to Sellers and buyers**-Solving problem statements based on Feature Importance"""

xgb_regressor.feature_importances_

df.columns

sorted_idx = xgb_regressor.feature_importances_.argsort()
plt.figure(figsize=(10,5))
plt.barh(df.columns[sorted_idx], xgb_regressor.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance")
plt.title("Feature Importance")
plt.show()

xgb_regressor.feature_importances_

sorted_idx = xgb_regressor.feature_importances_.argsort()
plt.figure(figsize=(10,5))
plt.barh(df.columns[sorted_idx], xgb_regressor.feature_importances_[sorted_idx])
plt.xlabel("Extreme Gradient Boosting Feature Importance")
plt.title("Feature Importance")
plt.show()

"""**Suggestion to Sellers**

Based on the **feature importance** given by Best Machine Learning
Algorithm(**Extreme Gradient Boosting**)-Sellers should Focus on the features of order given below to build homes that fetch more money:



1.   Age of the building,
2.   Area(location) of the building,
3.   No. of Rooms present,
4.   Build_type(Commercial,House etc..)-Commercial is best,
5.   MZ Zone,
6.   Parking Facility,
7.   No. of Bedrooms,
8.   Area(SQFT) of the building,
9.   No. of Bathrooms,
10.  Street(Paved, Gravel, No access, etc.) of the building,
11.  Sales Condition,
12.  Utility available.
"""